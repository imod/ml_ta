{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The input file contains the following fields:__\n",
    "\n",
    "| Name    | Description |\n",
    "| -------- | ------- |\n",
    "| UID  | Unique ID of the owning application    |\n",
    "| DBType | Type of the Databse (MSSQL, MySQL, Oracle, DB2)     |\n",
    "| Instance    | The instance of the Ddtabase |\n",
    "| DBName | The name of the database |\n",
    "| Schema | The schema where the table os located in the database |\n",
    "| Table | The name of the table |\n",
    "| Column | The name of a single column |\n",
    "| ColumnType | The datatyp of the column |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| DB | UID |    DBType | Instance |    DBName |      Schema |      Table |           Column |         ColumnType | \n",
    "| ----- | ----- |    ----- | ----- |    ----- |      ----- |      ----- |           ----- |         ----- | \n",
    "| __MSSQL__ | UID |    DBType | Instance |    DBName |      Schema |      Table |           Column |         ColumnType | \n",
    "| __Oracle__ | uid |   dbtype |   |           dbname |      owner |       table_name |      column_name |     data_type |\n",
    "| __MySQL__ | uid |    dbtype |    |          dbname |      owner |       table_name |      column_name |     data_type | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis\n",
    "!pip install wordninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "we have metadata about MSSQL, Oracle and MySQL Instances in CSV compressed in ZIP files - these CSVs must be merged into the same structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/mssql_allprod_collumns.zip\n",
      "  inflating: data/mssql_allprod_collumns.csv  \n",
      "Archive:  data/oracle_metadata.zip\n",
      "  inflating: data/oracle_metadata.csv  \n",
      "Archive:  data/mysql_metadata.zip\n",
      "  inflating: data/mysql_metadata.csv  \n"
     ]
    }
   ],
   "source": [
    "# remove the existing data files\n",
    "!rm -f data/*.csv  \n",
    "\n",
    "# unzip the data files avialable\n",
    "!unzip -o data/mssql_allprod_collumns.zip -d data/\n",
    "!unzip -o data/oracle_metadata.zip -d data/\n",
    "!unzip -o data/mysql_metadata.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID,DBType,Instance,DBName,Schema,Table,Column,ColumnType\n",
      "30230,MSSQL,R0015702\\RCHSCOMP02,OperationsManagerDW,State,StateHourly_D461F6AFA87B4259B908B29DA01EDE5C,InRedStateMilliseconds,int\n",
      "30230,MSSQL,R0015702\\RCHSCOMP02,OperationsManagerDW,State,StateHourly_D461F6AFA87B4259B908B29DA01EDE5C,InYellowStateMilliseconds,int\n",
      "The file data/mssql_allprod_collumns.csv has been processed and saved as data/mssql_allprod_collumns_no_third_column.csv.tmp\n",
      "The file data/mssql_allprod_collumns.csv has been processed, the third column has been removed:\n",
      "ï»¿UID,DBType,DBName,Schema,Table,Column,ColumnType\n",
      "30230,MSSQL,OperationsManagerDW,State,StateHourly_D461F6AFA87B4259B908B29DA01EDE5C,InRedStateMilliseconds,int\n",
      "30230,MSSQL,OperationsManagerDW,State,StateHourly_D461F6AFA87B4259B908B29DA01EDE5C,InYellowStateMilliseconds,int\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Prepare MSSQL data\n",
    "\n",
    "INPUT_FILE=\"data/mssql_allprod_collumns.csv\"\n",
    "OUTPUT_FILE=\"data/mssql_allprod_collumns_no_third_column.csv.tmp\"\n",
    "\n",
    "head -3 $INPUT_FILE\n",
    "\n",
    "# because the column separator is ',', we need to make sure e.g. `numeric(17,10)` is replaced with `numeric(17;10)` in the csv file\n",
    "sed -E 's/\\(([^)]*),([^)]*)\\)/(\\1;\\2)/g' ${INPUT_FILE} > ${OUTPUT_FILE}\n",
    "echo \"The file ${INPUT_FILE} has been processed and saved as ${OUTPUT_FILE}\"\n",
    "\n",
    "# dropo the third column ('Instance') from the csv file (it does not exist in the other files)\n",
    "# Use awk to drop the third column\n",
    "awk -F, 'BEGIN {OFS=\",\"} { $3=\"\"; sub(\",,\", \",\"); print }' ${OUTPUT_FILE} > ${INPUT_FILE}\n",
    "\n",
    "rm ${OUTPUT_FILE}\n",
    "\n",
    "echo \"The file ${INPUT_FILE} has been processed, the third column has been removed:\"\n",
    "head -3 $INPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_FILE=\"data/oracle_metadata.csv\"\n",
    "OUTPUT_FILE=\"data/oracle_metadata.csv.tmp\"\n",
    "# sed '/das den Zins verursacht hat/d' $INPUT_FILE > $OUTPUT_FILE\n",
    "sed '/RFA772203/d' $INPUT_FILE > $OUTPUT_FILE\n",
    "mv $OUTPUT_FILE $INPUT_FILE\n",
    "rm -f $OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# create the dataset to use for the analysis\n",
    "\n",
    "# the number of records to use for each DB type, set to '-1' to use all records\n",
    "NR_OF_RECORDS_EACH=-1\n",
    "\n",
    "TARGET_FILE=\"data/db_metadata.csv\"\n",
    "echo \"Creating the dataset...\"\n",
    "\n",
    "# drop the first line (column names) from the files (except the one from MSSQL)\n",
    "tail -n +2 data/oracle_metadata.csv > data/oracle_metadata.csv.tmp\n",
    "tail -n +2 data/mysql_metadata.csv > data/mysql_metadata.csv.tmp\n",
    "\n",
    "mv data/oracle_metadata.csv.tmp data/oracle_metadata.csv\n",
    "mv data/mysql_metadata.csv.tmp data/mysql_metadata.csv\n",
    "\n",
    "if [ $NR_OF_RECORDS_EACH -gt 0 ]; then\n",
    "    echo \"Creating a small dataset with ${NR_OF_RECORDS_EACH} records of each files\"\n",
    "    head -${NR_OF_RECORDS_EACH} data/mssql_allprod_collumns.csv  >  ${TARGET_FILE}\n",
    "    head -${NR_OF_RECORDS_EACH} data/oracle_metadata.csv         >> ${TARGET_FILE}\n",
    "    head -${NR_OF_RECORDS_EACH} data/mysql_metadata.csv          >> ${TARGET_FILE}\n",
    "else\n",
    "    echo \"Creating a dataset with all records of each files\"\n",
    "    cat data/mssql_allprod_collumns.csv data/oracle_metadata.csv data/mysql_metadata.csv > ${TARGET_FILE}\n",
    "fi\n",
    "\n",
    "NR_OF_LINES=$(wc -l ${TARGET_FILE})\n",
    "echo \"new dataset contains ${NR_OF_LINES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import wordninja\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>DBType</th>\n",
       "      <th>DBName</th>\n",
       "      <th>Schema</th>\n",
       "      <th>Table</th>\n",
       "      <th>Column</th>\n",
       "      <th>ColumnType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30230</td>\n",
       "      <td>MSSQL</td>\n",
       "      <td>OperationsManagerDW</td>\n",
       "      <td>State</td>\n",
       "      <td>StateHourly_D461F6AFA87B4259B908B29DA01EDE5C</td>\n",
       "      <td>InRedStateMilliseconds</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30230</td>\n",
       "      <td>MSSQL</td>\n",
       "      <td>OperationsManagerDW</td>\n",
       "      <td>State</td>\n",
       "      <td>StateHourly_D461F6AFA87B4259B908B29DA01EDE5C</td>\n",
       "      <td>InYellowStateMilliseconds</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30230</td>\n",
       "      <td>MSSQL</td>\n",
       "      <td>OperationsManagerDW</td>\n",
       "      <td>State</td>\n",
       "      <td>StateHourly_D461F6AFA87B4259B908B29DA01EDE5C</td>\n",
       "      <td>InDisabledStateMilliseconds</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30230</td>\n",
       "      <td>MSSQL</td>\n",
       "      <td>OperationsManagerDW</td>\n",
       "      <td>State</td>\n",
       "      <td>StateHourly_D461F6AFA87B4259B908B29DA01EDE5C</td>\n",
       "      <td>InPlannedMaintenanceMilliseconds</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30230</td>\n",
       "      <td>MSSQL</td>\n",
       "      <td>OperationsManagerDW</td>\n",
       "      <td>State</td>\n",
       "      <td>StateHourly_D461F6AFA87B4259B908B29DA01EDE5C</td>\n",
       "      <td>InUnplannedMaintenanceMilliseconds</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UID DBType               DBName Schema  \\\n",
       "0  30230  MSSQL  OperationsManagerDW  State   \n",
       "1  30230  MSSQL  OperationsManagerDW  State   \n",
       "2  30230  MSSQL  OperationsManagerDW  State   \n",
       "3  30230  MSSQL  OperationsManagerDW  State   \n",
       "4  30230  MSSQL  OperationsManagerDW  State   \n",
       "\n",
       "                                          Table  \\\n",
       "0  StateHourly_D461F6AFA87B4259B908B29DA01EDE5C   \n",
       "1  StateHourly_D461F6AFA87B4259B908B29DA01EDE5C   \n",
       "2  StateHourly_D461F6AFA87B4259B908B29DA01EDE5C   \n",
       "3  StateHourly_D461F6AFA87B4259B908B29DA01EDE5C   \n",
       "4  StateHourly_D461F6AFA87B4259B908B29DA01EDE5C   \n",
       "\n",
       "                               Column ColumnType  \n",
       "0              InRedStateMilliseconds        int  \n",
       "1           InYellowStateMilliseconds        int  \n",
       "2         InDisabledStateMilliseconds        int  \n",
       "3    InPlannedMaintenanceMilliseconds        int  \n",
       "4  InUnplannedMaintenanceMilliseconds        int  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('data/db_metadata.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop columns of known special tables\n",
    "df = df[~df['TABLE_NAME'].str.contains('flyway_schema_history')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the column names into words and drop short words\n",
    "df['document_content'] = df['Column'].apply(\n",
    "    lambda x: ' '.join([word for word in wordninja.split(x) if len(word) > 2])\n",
    ")\n",
    "\n",
    "print(df['document_content'].value_counts())\n",
    "# df = df.drop(columns=['Column'], inplace=False)\n",
    "\n",
    "# Concatenate 'Column' values for rows with the same 'DBName'/'Table' combination\n",
    "df_grouped = df.groupby(['UID', 'DBType', 'DBName', 'Table'])['document_content'].agg(\n",
    "    lambda x: ' '.join(x)\n",
    ").reset_index()\n",
    "\n",
    "print(df_grouped['document_content'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stopwords_list = stopwords.words('english') + stopwords.words('german')\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=final_stopwords_list, min_df=3, max_df=0.75, max_features=5000)\n",
    "doc_term_matrix = vectorizer.fit_transform(df_grouped['document_content'])\n",
    "\n",
    "print(doc_term_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=num_clusters, learning_method='online', random_state=42)\n",
    "doc_cluster_matrix = lda.fit_transform(doc_term_matrix)\n",
    "\n",
    "col_names = ['Cluster' + str(i) for i in range(num_clusters)]\n",
    "\n",
    "doc_cluster_df = pd.DataFrame(doc_cluster_matrix, columns=col_names)\n",
    "display(doc_cluster_df.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top words for each cluster\n",
    "num_words = 10\n",
    "\n",
    "for cluster, words in enumerate(lda.components_):\n",
    "    word_total = words.sum()\n",
    "    sorted_words = words.argsort()[::-1]\n",
    "    print()\n",
    "    print(f'Cluster {cluster}:')\n",
    "    for i in range(0, num_words):\n",
    "        word = vectorizer.get_feature_names_out()[sorted_words[i]]\n",
    "        word_wight = words[sorted_words[i]]\n",
    "        print(f'{word} {word_wight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize and analyze reuslts\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "\n",
    "prepared_data = pyLDAvis.lda_model.prepare(lda, doc_term_matrix, vectorizer, mds='tsne', sort_topics=False, n_jobs = -1)\n",
    "word_info = prepared_data.topic_info\n",
    "\n",
    "#Print top 30 keywords\n",
    "for topic in word_info.loc[word_info.Category != 'Default'].Category.unique():\n",
    "    print(topic)\n",
    "    print(word_info.loc[word_info.Category.isin([topic])].sort_values('logprob', ascending = False).Term.values[:30])\n",
    "    print()\n",
    "\n",
    "#To save prepared_data in an html panel (using https://panel.holoviz.org/)\n",
    "pyLDAvis.save_html(prepared_data, 'panel.html')\n",
    "print(\"Panel saved to panel.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "df['Column'] = df['Column'].str.lower()\n",
    "df = df[df['Column'].notna() & (df['Column'].str.strip() != '')]\n",
    "df['Table'] = df['Table'].str.lower()\n",
    "\n",
    "# Concatenate Schema and DBName columns\n",
    "df['DBName_Schema'] = df['Schema'] + '_' + df['DBName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dFrame.head(5))\n",
    "# print(dFrame.describe())\n",
    "print(df.info())\n",
    "print(df.isnull().sum())\n",
    "# print(df.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df['DBName_Schema'].value_counts()\n",
    "print(unique_counts)\n",
    "print(df['DBName_Schema'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['UID', 'Schema', 'DBName'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering using KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns likely to contain sensitive information\n",
    "sensitive_keywords = ['ssn', 'social', 'security', 'dob', 'date_of_birth', 'credit', 'card', 'ccn', 'account', 'bank', 'email', 'phone', 'address']\n",
    "df['Sensitive'] = df['Column'].apply(lambda x: any(keyword in x for keyword in sensitive_keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag potential compliance issues\n",
    "def flag_compliance_issues(column_name):\n",
    "    patterns = {\n",
    "        'PII': ['ssn', 'social', 'security', 'dob', 'date_of_birth', 'email', 'phone', 'address'],\n",
    "        'Financial': ['credit', 'card', 'ccn', 'account', 'bank']\n",
    "    }\n",
    "    for issue, keywords in patterns.items():\n",
    "        if any(keyword in column_name for keyword in keywords):\n",
    "            return issue\n",
    "    return 'None'\n",
    "\n",
    "df['Compliance_Issue'] = df['Column'].apply(flag_compliance_issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "df.to_csv('rai_allprod_collumns_with_clusters_and_flags.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
